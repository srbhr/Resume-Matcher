services:
  resume-matcher:
    build:
      context: .
      dockerfile: Dockerfile
    image: resume-matcher
    container_name: resume-matcher
    # Port mapping: HOST_PORT:CONTAINER_PORT
    # Customize host ports via environment variables (container ports are fixed)
    ports:
      - "${FRONTEND_PORT:-3000}:3000"   # Frontend (Next.js)
      - "${BACKEND_PORT:-8000}:8000"    # Backend API (FastAPI)
    volumes:
      - resume-data:/app/backend/data
    environment:
      - NODE_ENV=production
      # Pass port configuration to container for internal use
      - FRONTEND_PORT=${FRONTEND_PORT:-3000}
      - BACKEND_PORT=${BACKEND_PORT:-8000}
      # LLM Configuration - configure via Settings UI or set env vars for explicit overrides
      # Supported providers: openai, anthropic, openrouter, gemini, deepseek, ollama
      # Defaults are defined in apps/backend/app/config.py
      - LLM_PROVIDER=${LLM_PROVIDER:-}
      - LLM_MODEL=${LLM_MODEL:-}
      - LLM_API_KEY=${LLM_API_KEY:-}
      # For Ollama running on host machine, use host.docker.internal:
      # - LLM_API_BASE=http://host.docker.internal:11434
      - LLM_API_BASE=${LLM_API_BASE:-}
    restart: unless-stopped
    healthcheck:
      # Health check uses internal container port (always 8000)
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  resume-data:
    driver: local
