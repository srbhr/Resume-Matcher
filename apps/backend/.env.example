# ===========================================
# LLM Provider Configuration
# ===========================================
# Supported providers: openai, anthropic, openrouter, gemini, deepseek, ollama
LLM_PROVIDER=openai
LLM_MODEL=gpt-5-nano-2025-08-07
LLM_API_KEY=sk-your-api-key-here

# For Ollama (local models)
# LLM_PROVIDER=ollama
# LLM_MODEL=gemma3:4b
# LLM_API_BASE=http://localhost:11434
#
# IMPORTANT: For Docker deployments with Ollama on host machine:
# Use host.docker.internal instead of localhost:
# LLM_API_BASE=http://host.docker.internal:11434
# (Works on macOS/Windows. On Linux, use your host IP or --network=host)

# For OpenRouter
# LLM_PROVIDER=openrouter
# LLM_MODEL=deepseek/deepseek-v3.2
# LLM_API_KEY=sk-or-v1-your-key

# For Anthropic
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-haiku-4-5-20251001
# LLM_API_KEY=sk-ant-your-key

# For Google Gemini
# LLM_PROVIDER=gemini
# LLM_MODEL=gemini/gemini-3-flash-preview
# LLM_API_KEY=your-gemini-key

# For DeepSeek
# LLM_PROVIDER=deepseek
# LLM_MODEL=deepseek/deepseek-v3.2
# LLM_API_KEY=your-deepseek-key

# ===========================================
# Server Configuration
# ===========================================
HOST=0.0.0.0
PORT=8000

# Frontend URL - Used for PDF generation
FRONTEND_BASE_URL=http://localhost:3000

# ===========================================
# CORS Configuration
# ===========================================
CORS_ORIGINS=["http://localhost:3000","http://127.0.0.1:3000"]
